use serde::{Deserialize, Serialize};
use reqwest::Client;
use rand::Rng;

#[derive(Debug, Serialize, Deserialize)]
pub struct AIRequest {
    pub prompt: String,
    pub max_tokens: u32,
    pub temperature: f32,
    pub model: String,
    pub stream: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AIResponse {
    pub content: String,
    pub model: String,
    pub usage: TokenUsage,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TokenUsage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
    pub estimated_cost: Option<f64>,
}

// ChatGLM API è¯·æ±‚ç»“æ„ (å…¼å®¹ Anthropic æ ¼å¼)
#[derive(Debug, Serialize)]
struct ChatGLMRequest {
    model: String,
    messages: Vec<ChatMessage>,
    temperature: f32,
    max_tokens: u32,
    stream: bool,
}

#[derive(Debug, Serialize)]
struct ChatMessage {
    role: String,
    content: String,
}

// ChatGLM API å“åº”ç»“æ„
#[derive(Debug, Deserialize)]
struct ChatGLMResponse {
    id: Option<String>,
    choices: Vec<Choice>,
    model: String,
    usage: Option<Usage>,
    #[serde(flatten)]
    _extra: serde_json::Value, // å…¼å®¹ä¸åŒå“åº”æ ¼å¼
}

#[derive(Debug, Deserialize)]
struct Choice {
    index: Option<u32>,
    message: Option<Message>,
    delta: Option<Delta>,
    finish_reason: Option<String>,
}

#[derive(Debug, Deserialize)]
struct Message {
    role: Option<String>,
    content: Option<String>,
}

#[derive(Debug, Deserialize)]
struct Delta {
    content: Option<String>,
}

#[derive(Debug, Deserialize)]
struct Usage {
    prompt_tokens: u32,
    completion_tokens: u32,
    total_tokens: u32,
}

#[tauri::command]
pub async fn generate_ai_suggestion(
    request: AIRequest,
    api_key: Option<String>,
    api_base_url: Option<String>,
) -> Result<AIResponse, String> {
    // æ£€æŸ¥æ˜¯å¦é…ç½®äº† API Key
    let api_key = match api_key {
        Some(key) if !key.is_empty() => key,
        _ => {
            let error_msg = "æœªé…ç½® API Keyï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®";
            #[cfg(debug_assertions)]
            println!("âš ï¸  {}", error_msg);
            return Err(error_msg.to_string());
        }
    };

    let api_base = api_base_url.unwrap_or_else(||
        "https://open.bigmodel.cn/api/paas/v4/chat/completions".to_string()
    );

    #[cfg(debug_assertions)]
    {
        println!("ğŸš€ Calling AI API: {}", api_base);
        println!("ğŸ“ Model: {}", request.model);
    }

    let client = Client::new();

    // æ„å»ºè¯·æ±‚
    let chat_request = ChatGLMRequest {
        model: request.model.clone(),
        messages: vec![
            ChatMessage {
                role: "user".to_string(),
                content: request.prompt.clone(),
            }
        ],
        temperature: request.temperature,
        max_tokens: request.max_tokens,
        stream: false,
    };

    // å‘é€è¯·æ±‚
    let response = client
        .post(&api_base)
        .header("Authorization", format!("Bearer {}", api_key))
        .header("Content-Type", "application/json")
        .header("x-api-key", &api_key) // Anthropic æ ¼å¼éœ€è¦
        .header("anthropic-version", "2023-06-01") // Anthropic ç‰ˆæœ¬å¤´
        .json(&chat_request)
        .send()
        .await
        .map_err(|e| format!("APIè¯·æ±‚å¤±è´¥: {}", e))?;

    let status = response.status();
    let response_text = response.text().await.unwrap_or_default();

    if !status.is_success() {
        #[cfg(debug_assertions)]
        println!("âŒ APIè¿”å›é”™è¯¯: {} - {}", status, response_text);
        return Err(format!("APIè¿”å›é”™è¯¯ ({}): {}", status, response_text));
    }

    // è§£æå“åº”
    let chat_response: ChatGLMResponse = serde_json::from_str(&response_text)
        .map_err(|e| {
            #[cfg(debug_assertions)]
            {
                println!("âŒ è§£æå“åº”å¤±è´¥: {}", e);
                println!("ğŸ“„ å“åº”å†…å®¹: {}", response_text);
            }
            format!("è§£æå“åº”å¤±è´¥: {}", e)
        })?;

    // æå–å†…å®¹
    let content = if let Some(choice) = chat_response.choices.first() {
        if let Some(msg) = &choice.message {
            if let Some(c) = &msg.content {
                c.clone()
            } else if let Some(delta) = &choice.delta {
                delta.content.clone().unwrap_or_default()
            } else {
                String::new()
            }
        } else {
            String::new()
        }
    } else {
        return Err("APIè¿”å›ç©ºå“åº”".to_string());
    };

    if content.is_empty() {
        return Err("APIè¿”å›å†…å®¹ä¸ºç©º".to_string());
    }

    let usage = chat_response.usage.unwrap_or(Usage {
        prompt_tokens: 0,
        completion_tokens: 0,
        total_tokens: 0,
    });

    #[cfg(debug_assertions)]
    println!("âœ… APIè°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆå†…å®¹é•¿åº¦: {} å­—ç¬¦", content.len());

    Ok(AIResponse {
        content,
        model: chat_response.model,
        usage: TokenUsage {
            prompt_tokens: usage.prompt_tokens,
            completion_tokens: usage.completion_tokens,
            total_tokens: usage.total_tokens,
            estimated_cost: None,
        },
    })
}

// Mock å“åº”ç”Ÿæˆå™¨ï¼ˆå½“ API æœªé…ç½®æ—¶ä½¿ç”¨ï¼‰
async fn generate_mock_response(request: &AIRequest) -> Result<AIResponse, String> {
    let mock_suggestions = vec![
        "å¤œå¹•é™ä¸´ï¼ŒåŸå¸‚çš„éœ“è™¹ç¯å¼€å§‹é—ªçƒï¼Œè¡—é“ä¸Šçš„è¡Œäººæ¸æ¸ç¨€å°‘ã€‚",
        "å¾®é£å¹è¿‡ï¼Œå¸¦æ¥äº†è¿œæ–¹çš„èŠ±é¦™ï¼Œä¹Ÿå¹èµ·äº†å¿ƒä¸­çš„æ¶Ÿæ¼ªã€‚",
        "é›¨æ»´æ•²æ‰“ç€çª—æˆ·ï¼Œå‘å‡ºæ¸…è„†çš„å£°å“ï¼Œä»¿ä½›åœ¨è¯‰è¯´ç€ä»€ä¹ˆã€‚",
        "é˜³å…‰é€è¿‡äº‘å±‚çš„ç¼éš™æ´’å‘å¤§åœ°ï¼Œç»™è¿™ä¸ªæ¸…æ™¨å¸¦æ¥äº†æ¸©æš–ã€‚",
        "æœˆå…‰å¦‚æ°´èˆ¬æ´’åœ¨æ¹–é¢ä¸Šï¼Œæ³›èµ·å±‚å±‚é“¶è‰²çš„æ¶Ÿæ¼ªã€‚",
        "è¿œå±±å¦‚é»›ï¼Œè¿‘æ°´å«çƒŸï¼Œæ„æˆäº†ä¸€å¹…ç»ç¾çš„å±±æ°´ç”»å·ã€‚",
    ];

    let mut rng = rand::thread_rng();
    let selected_suggestion = mock_suggestions[rng.gen_range(0..mock_suggestions.len())];

    #[cfg(debug_assertions)]
    println!("ğŸ­ Using mock suggestion: {}", selected_suggestion);

    Ok(AIResponse {
        content: selected_suggestion.to_string(),
        model: request.model.clone(),
        usage: TokenUsage {
            prompt_tokens: 100,
            completion_tokens: 50,
            total_tokens: 150,
            estimated_cost: Some(0.002),
        },
    })
}